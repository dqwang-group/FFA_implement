# 测试笔记
## 1. 超参数的调整: 技巧 / 经验 / 直觉

- **1.1 宗旨:** 

- 经验为主; 好的 first-guess 基本依靠经验

- 参考 related-work 中别人选取的超参数, 作为自己的 first-guess

- 从小型模型开始测试超参数, 不会浪费大量时间

- 大的 grid-search 一般不现实; Try to **group hyperparameters**, and optimize each group one by one (即做小规模的, 多组的 grid-search) 

<br/>

- **1.2 Learning Rate:**

- The learning rate is an important parameter, which depends on the optimizer, the model, and many more other hyperparameters.

- A usual good starting point is **0.1 for SGD**, and **1e-3 for Adam**.

- The **deeper the model** is, the **lower the learning rate** usually should be. For instance, Transformer models usually apply learning rates of 1e-5 to 1e-4 for Adam.

- The **lower your batch**, the **lower the learning rate** should be. Consider using gradient accumulation if your batch size is getting too small (PyTorch Lightning supports this, see here).

- Consider using the PyTorch Lightning learning rate finder toolkit for an initial good guess.

<br/>

- **1.3 Regularization:**

- Regularization is important in networks if you see a significantly higher training performance than test performance.

- The regularization parameters all interact with each other, and hence must be tuned together. The most commonly used regularization techniques are:

    - Weight decay

    - Dropout

    - Augmentation

- Dropout is usually a good idea as it is applicable to most architectures and has shown to effectively reduce overfitting.

- If you want to use weight decay in Adam, remember to use torch.optim.AdamW instead of torch.optim.Adam.